{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c9d817-a78f-43bb-bec7-b6bcdef0fa16",
   "metadata": {},
   "source": [
    "# Final model performance\n",
    "\n",
    "In this notebook we run our best-performing model on the test set to get a final estimate of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a281496-874f-4c74-acb9-3d411a7e59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ecb91",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e4691",
   "metadata": {},
   "source": [
    "In `3_ModelSelection.ipynb`, we used cross-validation to select ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define/instantiate model from 3_ModelSelection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d5c19",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./train_data.csv')\n",
    "# df_test = pd.read_csv('./test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2a737",
   "metadata": {},
   "source": [
    "We will now perform some imputation and scaling. It is important to note that all of the transformations we will perform here will be performed \"within season,\" meaning that the data for a given season is transformed using information from *only* that season and no other.\n",
    "\n",
    "In particular, since at the time of prediction we will have access to all of the data for that particular season, there is ***no data leakage*** occurring here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac51d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    # fill all null stats with 0 (except for SALARY)\n",
    "    null_cols = df_train.count()[df_train.count() < len(df_train)].index.drop('SALARY')\n",
    "    df_train[null_cols] = df_train[null_cols].fillna(0)\n",
    "\n",
    "    # use mean imputer for SALARY within each season\n",
    "    df_train.loc[df_train['SALARY']==0, 'SALARY'] = None\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    df_train['SALARY'] = (\n",
    "        df_train\n",
    "        .groupby('SEASON_START')['SALARY']\n",
    "        .transform(lambda x: mean_imputer.fit_transform(x.values.reshape(-1,1)).ravel())\n",
    "    )\n",
    "\n",
    "    # use standard scaler within each season\n",
    "    cols_to_rescale = df_train.select_dtypes(include=['float']).columns\n",
    "    scaler = StandardScaler()\n",
    "    df_train[cols_to_rescale] = (\n",
    "        df_train\n",
    "        .groupby('SEASON_START')[cols_to_rescale]\n",
    "        .transform(lambda x: scaler.fit_transform(x.values.reshape(-1,1)).ravel())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4ba40",
   "metadata": {},
   "source": [
    "## Testing model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34d67d",
   "metadata": {},
   "source": [
    "In order to get the most realistic estimate of our model performance, we will perform walk-forward testing. The idea is as follows:\n",
    "\n",
    "0. **Initialize datasets**: Begin with training set `df_train` (seasons 1990-91 through 2016-17) and test set `df_test[df_test['SEASON_START']==2017]` (season 2017-18).\n",
    "\n",
    "1. **Train the model**: Fit the model using the current training set.\n",
    "\n",
    "2. **Evaluate performance**: Use the model to predict outcomes for the current test set and compute performance metrics.\n",
    "\n",
    "3. **Itereate**: Expand the training set to include the current test set, replace the test set with data from the next season, and repeat from Step 1 until no future data is available.\n",
    "\n",
    "See the following table for explicit details.\n",
    "\n",
    "| Iteration | Training set start seasons | Test set start season |\n",
    "| ---       | ---                        | ---                   |\n",
    "| 1         | 1990 - 2016                | 2017                  |\n",
    "| 2         | 1990 - 2017                | 2018                  |\n",
    "| 3         | 1990 - 2018                | 2019                  |\n",
    "| 4         | 1990 - 2019                | 2020                  |\n",
    "| 5         | 1990 - 2020                | 2021                  |\n",
    "| 6         | 1990 - 2021                | 2022                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d31091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform walk-forward testing, compute average performace metrics across each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f45df-2afa-4490-a663-974879f47f3d",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "**TODO** write up conclusions, make nice figures, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c59c0-4374-4e1e-97e4-c53fba94a7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
